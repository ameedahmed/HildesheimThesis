{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb8e766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bcolz\n",
      "  Downloading bcolz-1.2.1.tar.gz (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.5/1.5 MB 8.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting mxnet\n",
      "  Downloading mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl.metadata (402 bytes)\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (3.9.1.post1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting easydict\n",
      "  Downloading easydict-1.13-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: opencv-python in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.7 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from bcolz) (1.26.4)\n",
      "Collecting numpy>=1.7 (from bcolz)\n",
      "  Downloading numpy-1.16.6.zip (5.1 MB)\n",
      "     ---------------------------------------- 0.0/5.1 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 1.3/5.1 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 2.9/5.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.7/5.1 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 4.7/5.1 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.1/5.1 MB 5.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [24 lines of output]\n",
      "      Running from numpy source directory.\n",
      "      <string>:394: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
      "      C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-kntt75kv\\numpy_4017e8917c93442cae844d8dad0b7606\\numpy\\distutils\\misc_util.py:476: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "        return is_string(s) and ('*' in s or '?' is s)\n",
      "      Traceback (most recent call last):\n",
      "        File \"d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-wr39voue\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 374, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-wr39voue\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-wr39voue\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 419, in <module>\n",
      "        File \"<string>\", line 398, in setup_package\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-kntt75kv\\numpy_4017e8917c93442cae844d8dad0b7606\\numpy\\distutils\\__init__.py\", line 6, in <module>\n",
      "          from . import ccompiler\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-kntt75kv\\numpy_4017e8917c93442cae844d8dad0b7606\\numpy\\distutils\\ccompiler.py\", line 111, in <module>\n",
      "          replace_method(CCompiler, 'find_executables', CCompiler_find_executables)\n",
      "      NameError: name 'CCompiler' is not defined. Did you mean: 'ccompiler'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: imgaug in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: PyTurboJPEG in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (1.7.7)\n",
      "Requirement already satisfied: numpy>=1.24 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.11.4 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (1.15.3)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=10.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (10.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: six in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (3.9.1.post1)\n",
      "Requirement already satisfied: opencv-python in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (4.11.0.86)\n",
      "Requirement already satisfied: Shapely in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (2.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (2.9.0.post0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.7.1+cu110 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu113, 1.11.0+cu115, 1.12.0, 1.12.0+cpu, 1.12.0+cu113, 1.12.0+cu116, 1.12.1, 1.12.1+cpu, 1.12.1+cu113, 1.12.1+cu116, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 2.0.0, 2.0.0+cpu, 2.0.0+cu117, 2.0.0+cu118, 2.0.1, 2.0.1+cpu, 2.0.1+cu117, 2.0.1+cu118, 2.1.0, 2.1.0+cpu, 2.1.0+cu118, 2.1.0+cu121, 2.1.1, 2.1.1+cpu, 2.1.1+cu118, 2.1.1+cu121, 2.1.2, 2.1.2+cpu, 2.1.2+cu118, 2.1.2+cu121, 2.2.0, 2.2.0+cpu, 2.2.0+cu118, 2.2.0+cu121, 2.2.1, 2.2.1+cpu, 2.2.1+cu118, 2.2.1+cu121, 2.2.2, 2.2.2+cpu, 2.2.2+cu118, 2.2.2+cu121, 2.3.0, 2.3.0+cpu, 2.3.0+cu118, 2.3.0+cu121, 2.3.1, 2.3.1+cpu, 2.3.1+cu118, 2.3.1+cu121, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for torch==1.7.1+cu110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: imgaug in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: prettytable in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (3.16.0)\n",
      "Requirement already satisfied: six in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (1.15.3)\n",
      "Requirement already satisfied: Pillow in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (3.9.1.post1)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (0.25.2)\n",
      "Requirement already satisfied: opencv-python in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (4.11.0.86)\n",
      "Requirement already satisfied: imageio in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (2.37.0)\n",
      "Requirement already satisfied: Shapely in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from imgaug) (2.1.0)\n",
      "Requirement already satisfied: wcwidth in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from prettytable) (0.2.13)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (3.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from matplotlib->imgaug) (2.9.0.post0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm==0.3.4 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: torch>=1.4 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from timm==0.3.4) (2.2.0)\n",
      "Requirement already satisfied: torchvision in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from timm==0.3.4) (0.17.0)\n",
      "Requirement already satisfied: filelock in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torch>=1.4->timm==0.3.4) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torch>=1.4->timm==0.3.4) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torch>=1.4->timm==0.3.4) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torch>=1.4->timm==0.3.4) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torch>=1.4->timm==0.3.4) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torch>=1.4->timm==0.3.4) (2024.6.1)\n",
      "Requirement already satisfied: numpy in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torchvision->timm==0.3.4) (1.26.4)\n",
      "Requirement already satisfied: requests in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torchvision->timm==0.3.4) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from torchvision->timm==0.3.4) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from jinja2->torch>=1.4->timm==0.3.4) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from requests->torchvision->timm==0.3.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from requests->torchvision->timm==0.3.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from requests->torchvision->timm==0.3.4) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from requests->torchvision->timm==0.3.4) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\nco_code\\nco_code-main-single_objective-lehd-tsp\\single_objective\\lehd\\tsp\\.venv\\lib\\site-packages (from sympy->torch>=1.4->timm==0.3.4) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: d:\\nco_code\\NCO_code-main-single_objective-LEHD-TSP\\single_objective\\LEHD\\TSP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install all requirements in Jupyter Notebook cells\n",
    "!pip install bcolz mxnet tensorboardX matplotlib easydict opencv-python einops --no-cache-dir -U\n",
    "!pip install scikit-image imgaug PyTurboJPEG --no-cache-dir -U\n",
    "!pip install scikit-learn --no-cache-dir -U\n",
    "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html --no-cache-dir -U\n",
    "!pip install termcolor imgaug prettytable --no-cache-dir -U\n",
    "!pip install timm==0.3.4 --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c46ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in d:\\thesis\\.venv\\lib\\site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install einops --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719ac4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\thesis\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\thesis\\.venv\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe3a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for cv2\n"
     ]
    }
   ],
   "source": [
    "pip install cv2 --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e786fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in d:\\thesis\\.venv\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in d:\\thesis\\.venv\\lib\\site-packages (from timm) (2.2.0)\n",
      "Requirement already satisfied: torchvision in d:\\thesis\\.venv\\lib\\site-packages (from timm) (0.17.0)\n",
      "Requirement already satisfied: pyyaml in d:\\thesis\\.venv\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in d:\\thesis\\.venv\\lib\\site-packages (from timm) (0.31.2)\n",
      "Requirement already satisfied: safetensors in d:\\thesis\\.venv\\lib\\site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\thesis\\.venv\\lib\\site-packages (from huggingface_hub->timm) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\thesis\\.venv\\lib\\site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\thesis\\.venv\\lib\\site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in d:\\thesis\\.venv\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\thesis\\.venv\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\thesis\\.venv\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\thesis\\.venv\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\thesis\\.venv\\lib\\site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\thesis\\.venv\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in d:\\thesis\\.venv\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\thesis\\.venv\\lib\\site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: colorama in d:\\thesis\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\thesis\\.venv\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\thesis\\.venv\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\thesis\\.venv\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\thesis\\.venv\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\thesis\\.venv\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\thesis\\.venv\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dea9326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting models\n",
      "  Using cached models-0.9.3.tar.gz (16 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [19 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"d:\\Thesis\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"d:\\Thesis\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"d:\\Thesis\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-lsuaq5e5\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-lsuaq5e5\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-lsuaq5e5\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-lsuaq5e5\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 25, in <module>\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-pg4dldt0\\models_7ffc94fb5cd54b02845fdbc046c2b3ec\\models\\__init__.py\", line 23, in <module>\n",
      "          from base import *\n",
      "      ModuleNotFoundError: No module named 'base'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc550fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Thesis\\.venv\\lib\\site-packages\\timm\\models\\helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "d:\\Thesis\\.venv\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "d:\\Thesis\\.venv\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from timm.models import create_model\n",
    "from models import CSWin_64_12211_tiny_224  # Adjust if your model class is named differently\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3c024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "['stage1_conv_embed.0.weight', 'stage1_conv_embed.0.bias', 'stage1_conv_embed.2.weight', 'stage1_conv_embed.2.bias', 'stage1.0.qkv.weight', 'stage1.0.qkv.bias', 'stage1.0.norm1.weight', 'stage1.0.norm1.bias', 'stage1.0.proj.weight', 'stage1.0.proj.bias', 'stage1.0.attns.0.get_v.weight', 'stage1.0.attns.0.get_v.bias', 'stage1.0.attns.1.get_v.weight', 'stage1.0.attns.1.get_v.bias', 'stage1.0.mlp.fc1.weight', 'stage1.0.mlp.fc1.bias', 'stage1.0.mlp.fc2.weight', 'stage1.0.mlp.fc2.bias', 'stage1.0.norm2.weight', 'stage1.0.norm2.bias']\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the model instance (do not use pretrained=True unless you want to load default weights)\n",
    "model = CSWin_64_12211_tiny_224(pretrained=False)\n",
    "\n",
    "# 2. Load the checkpoint\n",
    "checkpoint = torch.load('cswin_tiny_224.pth', map_location='cpu')\n",
    "\n",
    "# 3. Extract the state_dict (handles both plain and wrapped checkpoints)\n",
    "if 'state_dict_ema' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict_ema']\n",
    "elif 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# 4. Remove 'module.' prefix if present (for DataParallel checkpoints)\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "# 5. Load weights into the model\n",
    "model.load_state_dict(new_state_dict, strict=False)  # strict=False ignores non-matching keys\n",
    "\n",
    "missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "print(list(state_dict.keys())[:20])  # Print first 20 keys\n",
    "# 6. Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 5. (Optional) Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eadb3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 404\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess your image\n",
    "img = Image.open('D:/Thesis/airplane.jpg').convert('RGB')\n",
    "transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "#input_tensor = input_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor) # model inference mode\n",
    "    pred = output.argmax(dim=1)\n",
    "    print('Predicted class:', pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7de4f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class name: airliner\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "# Download ImageNet class index mapping if you don't have it\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "filename = \"imagenet_classes.txt\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Read the class names\n",
    "with open(filename, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(\"Predicted class name:\", classes[pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b179dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Thesis\\.venv\\lib\\site-packages\\timm\\models\\helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "d:\\Thesis\\.venv\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "d:\\Thesis\\.venv\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "d:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:437: UserWarning: Overwriting CSWin_64_12211_tiny_224 in registry with models.cswinmodified.CSWin_64_12211_tiny_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def CSWin_64_12211_tiny_224(pretrained=False, **kwargs):\n",
      "d:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:444: UserWarning: Overwriting CSWin_64_24322_small_224 in registry with models.cswinmodified.CSWin_64_24322_small_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def CSWin_64_24322_small_224(pretrained=False, **kwargs):\n",
      "d:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:451: UserWarning: Overwriting CSWin_96_24322_base_224 in registry with models.cswinmodified.CSWin_96_24322_base_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def CSWin_96_24322_base_224(pretrained=False, **kwargs):\n",
      "d:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:458: UserWarning: Overwriting CSWin_144_24322_large_224 in registry with models.cswinmodified.CSWin_144_24322_large_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def CSWin_144_24322_large_224(pretrained=False, **kwargs):\n",
      "d:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:467: UserWarning: Overwriting CSWin_96_24322_base_384 in registry with models.cswinmodified.CSWin_96_24322_base_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def CSWin_96_24322_base_384(pretrained=False, **kwargs):\n",
      "d:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:474: UserWarning: Overwriting CSWin_144_24322_large_384 in registry with models.cswinmodified.CSWin_144_24322_large_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def CSWin_144_24322_large_384(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stage1_conv_embed.0.weight', 'stage1_conv_embed.0.bias', 'stage1_conv_embed.2.weight', 'stage1_conv_embed.2.bias', 'stage1.0.qkv.weight', 'stage1.0.qkv.bias', 'stage1.0.norm1.weight', 'stage1.0.norm1.bias', 'stage1.0.proj.weight', 'stage1.0.proj.bias', 'stage1.0.attns.0.get_v.weight', 'stage1.0.attns.0.get_v.bias', 'stage1.0.attns.1.get_v.weight', 'stage1.0.attns.1.get_v.bias', 'stage1.0.mlp.fc1.weight', 'stage1.0.mlp.fc1.bias', 'stage1.0.mlp.fc2.weight', 'stage1.0.mlp.fc2.bias', 'stage1.0.norm2.weight', 'stage1.0.norm2.bias']\n",
      "Predicted class: 404\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 226\u001b[0m\n\u001b[0;32m    223\u001b[0m     pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted class:\u001b[39m\u001b[38;5;124m'\u001b[39m, pred\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m--> 226\u001b[0m vis_global \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m)) \u001b[38;5;66;03m# 4 subplots: original + 3 maps\u001b[39;00m\n\u001b[0;32m    229\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "Cell \u001b[1;32mIn[1], line 112\u001b[0m, in \u001b[0;36mgenerate_visualization\u001b[1;34m(original_image, class_index)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_visualization\u001b[39m(original_image, class_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Get relevance maps for each attention type\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     R_vertical, R_horizontal, R_global \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     grid_h, grid_w \u001b[38;5;241m=\u001b[39m get_patch_grid_size(model, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Reshape and upsample for visualization\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m, in \u001b[0;36mgenerate_relevance\u001b[1;34m(model, input, index)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_relevance\u001b[39m(model,\u001b[38;5;28minput\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 32\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m     34\u001b[0m         output, global_attention, vertical_attention, horizontal_attention,overall_attention \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32md:\\Thesis\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Thesis\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:390\u001b[0m, in \u001b[0;36mCSWinTransformer.forward\u001b[1;34m(self, x, register_hook, return_attentions)\u001b[0m\n\u001b[0;32m    388\u001b[0m                     x \u001b[38;5;241m=\u001b[39m blk(x)\n\u001b[0;32m    389\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m                 x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregister_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m#Global For Stage 4                    x, attn_global = blk(x, register_hook=register_hook, return_attentions=True)\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m#Global For Stage 4                    attn_global_list.append(attn_global)\u001b[39;00m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pre, blocks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge3], \n\u001b[0;32m    396\u001b[0m                                [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage3, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage4]):\n",
      "File \u001b[1;32md:\\Thesis\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Thesis\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Thesis\\CSWin_Transformer_main\\models\\cswinmodified.py:228\u001b[0m, in \u001b[0;36mCSWinBlock.forward\u001b[1;34m(self, x, register_hook, return_attentions)\u001b[0m\n\u001b[0;32m    226\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattns[\u001b[38;5;241m0\u001b[39m](qkv[:,:,:,:C\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m], register_hook\u001b[38;5;241m=\u001b[39mregister_hook, return_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    227\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattns[\u001b[38;5;241m1\u001b[39m](qkv[:,:,:,C\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:], register_hook\u001b[38;5;241m=\u001b[39mregister_hook, return_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)            \n\u001b[1;32m--> 228\u001b[0m     attened_x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_attentions:\n\u001b[0;32m    232\u001b[0m     attened_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattns[\u001b[38;5;241m0\u001b[39m](qkv)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from timm.models import create_model\n",
    "from models.cswinmodified import CSWin_64_12211_tiny_224  # Use the modified model\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def avg_heads(cam,grad):\n",
    "    # cam is the attention \n",
    "    # grad is the gradient of the said attention.\n",
    "    cam = cam.reshape(-1, cam.shape[-2], cam.shape[-1])\n",
    "    grad = grad.reshape(-1, grad.shape[-2], grad.shape[-1])\n",
    "    cam = grad * cam\n",
    "    cam = cam.clamp(min=0).mean(dim=0)\n",
    "    return cam\n",
    "\n",
    "def apply_self_attention_rules(R_ss,cam_ss):\n",
    "    R_ss_addition = torch.matmul(cam_ss,R_ss)\n",
    "    return R_ss_addition\n",
    "\n",
    "def upsample_relevance_nearest(R,old_tokens, new_tokens):\n",
    "    R = R.reshape(1,1,old_tokens,old_tokens)\n",
    "    R = torch.nn.functional.interpolate(R, size=(new_tokens, new_tokens), mode='bilinear')\n",
    "    R = R.reshape(new_tokens, new_tokens)\n",
    "    return R\n",
    "\n",
    "def generate_relevance(model,input,index=None):\n",
    "    result = model(input, register_hook=True, return_attentions=True)\n",
    "    if len(result) == 5:\n",
    "        output, global_attention, vertical_attention, horizontal_attention,overall_attention = result\n",
    "        # Use all four\n",
    "    else:\n",
    "        output, global_attention = result\n",
    "        # Use only two\n",
    "    if index == None:\n",
    "        index=np.argmax(output.cpu().data.numpy(),axis=-1)\n",
    "    \n",
    "    one_hot = np.zeros((1,output.size()[-1]),dtype=np.float32)\n",
    "    one_hot[0, index] = 1\n",
    "    one_hot_vector = one_hot\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    one_hot = torch.sum(one_hot * output)\n",
    "    model.zero_grad()\n",
    "    one_hot.backward(retain_graph=True)\n",
    " \n",
    "    #num_tokens = vertical_attention[-1].shape[-1]\n",
    "#    else:\n",
    "    num_tokens = model.stage4[0].attns[0].get_attention_map().shape[-1]\n",
    "\n",
    "    # Initialize relevance matrices\n",
    "    \n",
    "    # Loop through all vertical/horizontal blocks (all but last stage)\n",
    "    # You may need to adjust this if your model has a different number of blocks per stage\n",
    "\n",
    "    # For the last stage (global attention)\n",
    "    blk = model.stage4[-1]\n",
    "    grad_g = blk.attns[0].get_attn_gradients()\n",
    "    cam_g = blk.attns[0].get_attention_map()\n",
    "    cam_g = avg_heads(cam_g, grad_g)\n",
    "    R_global = torch.eye(num_tokens, num_tokens)\n",
    "    R_global += apply_self_attention_rules(R_global, cam_g)\n",
    "\n",
    "    # Return relevance maps for each type\n",
    "    # Correct order: vertical, horizontal, global\n",
    "    return R_global.sum(dim=0) \n",
    "\n",
    "    # create heatmap from mask on image\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "def get_patch_grid_size(model, img_size=224):\n",
    "    # Try to get patch size from model, fallback to 4 if not present\n",
    "    patch_size = getattr(model, 'patch_size', 4)\n",
    "    if isinstance(patch_size, tuple):\n",
    "        patch_h, patch_w = patch_size\n",
    "    else:\n",
    "        patch_h = patch_w = patch_size\n",
    "    grid_h = img_size // patch_h\n",
    "    grid_w = img_size // patch_w\n",
    "    return grid_h, grid_w\n",
    "\n",
    "def generate_visualization(original_image, class_index=None):\n",
    "    # Get relevance maps for each attention type\n",
    "    R_global = generate_relevance(model, original_image.unsqueeze(0), index=class_index)\n",
    "    grid_h, grid_w = get_patch_grid_size(model, img_size=224)\n",
    "    # Reshape and upsample for visualization\n",
    "    def process_relevance(R):\n",
    "        R = R.detach()\n",
    "        numel = R.numel()\n",
    "        # Try to find two factors (h, w) such that h * w == numel and h <= w\n",
    "        best_h, best_w = 1, numel\n",
    "        min_diff = numel\n",
    "        for h in range(1, int(np.sqrt(numel)) + 1):\n",
    "            if numel % h == 0:\n",
    "                w = numel // h\n",
    "                if abs(w - h) < min_diff:\n",
    "                    best_h, best_w = h, w\n",
    "                    min_diff = abs(w - h)\n",
    "        # If not a perfect rectangle, pad to next (h, w)\n",
    "        if best_h * best_w != numel:\n",
    "            next_square = int(np.ceil(np.sqrt(numel))) ** 2\n",
    "            pad_len = next_square - numel\n",
    "            R = torch.nn.functional.pad(R, (0, pad_len))\n",
    "            best_h = best_w = int(np.sqrt(next_square))\n",
    "        R = R.reshape(1, 1, best_h, best_w)\n",
    "        R = torch.nn.functional.interpolate(R, size=(224, 224), mode='bilinear')\n",
    "        R = R.reshape(224, 224).data.cpu().numpy()\n",
    "        R = (R - R.min()) / (R.max() - R.min())\n",
    "        print(R)\n",
    "        return R\n",
    "\n",
    "    global_map = process_relevance(R_global)\n",
    "\n",
    "    image_np = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "\n",
    "    mask_resized = cv2.resize(global_map, (img.width, img.height))\n",
    "    vis_global = show_cam_on_image(np.array(img)/255.0, mask_resized)\n",
    "    #vis_global = show_cam_on_image(image_np, global_map)\n",
    "\n",
    "    vis_global = np.uint8(255 * vis_global)\n",
    "\n",
    "    vis_global = cv2.cvtColor(np.array(vis_global), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Return in the correct order for plotting: global, vertical, horizontal\n",
    "    return vis_global\n",
    "\n",
    "def print_top_classes(predictions, **kwargs):\n",
    "    # Print Top-5 predictions\n",
    "    prob = torch.softmax(predictions, dim=1)\n",
    "    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()\n",
    "    max_str_len = 0\n",
    "    class_names = []\n",
    "    for cls_idx in class_indices:\n",
    "        class_names.append(CLS2IDX[cls_idx])\n",
    "        if len(CLS2IDX[cls_idx]) > max_str_len:\n",
    "            max_str_len = len(CLS2IDX[cls_idx])\n",
    "\n",
    "# 1. Create the model instance (do not use pretrained=True unless you want to load default weights)\n",
    "model = CSWin_64_12211_tiny_224(pretrained=False)\n",
    "\n",
    "# 2. Load the checkpoint\n",
    "checkpoint = torch.load('cswin_tiny_224.pth', map_location='cpu')\n",
    "\n",
    "# 3. Extract the state_dict (handles both plain and wrapped checkpoints)\n",
    "if 'state_dict_ema' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict_ema']\n",
    "elif 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# 4. Remove 'module.' prefix if present (for DataParallel checkpoints)\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "# 5. Load weights into the model\n",
    "model.load_state_dict(new_state_dict, strict=False)  # strict=False ignores non-matching keys\n",
    "\n",
    "print(list(state_dict.keys())[:20])  # Print first 20 keys\n",
    "# 6. Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 5. (Optional) Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Load and preprocess your image\n",
    "img = Image.open('D:/Thesis/airplane.jpg').convert('RGB')\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    #T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "axs[0].imshow(img)\n",
    "axs[0].axis('off')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred = output.argmax(dim=1)\n",
    "    print('Predicted class:', pred.item())\n",
    "\n",
    "vis_global = generate_visualization(transform(img), class_index=1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5)) # 4 subplots: original + 3 maps\n",
    "axs[0].imshow(img)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Original')\n",
    "\n",
    "axs[1].imshow(vis_global)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Global')\n",
    "\n",
    "\n",
    "# Download ImageNet class index mapping if you don't have it\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "filename = \"imagenet_classes.txt\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Read the class names\n",
    "with open(filename, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(\"Predicted class name:\", classes[pred.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
